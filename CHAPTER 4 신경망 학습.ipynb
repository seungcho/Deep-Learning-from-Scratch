{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CHAPTER 4 신경망 학습**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**학습이란 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것을 뜻한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**신경망이 학습할 수 있도록 해주는 지표인 손실 함수를 소개한다.**\n",
    "\n",
    "> **이 손실 함수의 결괏값을 가장 작게 만드는 가중치 매개변수를 찾는 것이 학습의 목표이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1 데이터에서 학습한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**신경망의 특징은 데이터를 보고 학습할 수 있다는 점이다. 데이터에서 학습한다는 것은 가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다는 뜻이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1.1 데이터 주도 학습**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**기계학습은 데이터가 생명이다. 데이터에서 답을 찾고 데이터에서 패턴을 발견하고 데이터로 이야기를 만드는, 그것이 바로 기계학습이다.**\n",
    "\n",
    "> **기계학습의 중심에는 데이터가 존재한다. 이처럼 데이터가 이끄는 접근 방식 덕에 사람 중심 접근에서 벗어날 수 있다.**\n",
    "\n",
    "**보통 어떤 문제를 해결하려 들때, 특히 어떤 패턴을 찾아내야 할 때는 사람의 경험과 직관을 단서로 시행착오를 거듭하여 일을 진행한다.**\n",
    "\n",
    "> **반면 기계학습에서는 사람의 개입을 최소화하고 수집한 데이터로부터 패턴을 찾으려 시도한다.**\n",
    "\n",
    "**게다가 신경망과 딥러닝은 기존 기계학습에서 사용하던 방법보다 사람의 개입을 더욱 배제할 수 있게 해주는 중요한 특성을 지녔다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**기계학습에서는 모아진 데이터로부터 규칙을 찾아내는 역할을 '기계'가 담당한다.**\n",
    "\n",
    "> **다만, 이미지를 벡터로 변환할 때 사용하는 특징은 여전히 '사람'이 설계하는 것임에 주의해야 한다.**\n",
    "\n",
    "**즉, 특징과 기계학습을 활용한 접근에도 문제에 따라서는 '사람'이 적절한 특징을 생각해내야한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](./img/패러다임.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **두 번째 접근 방식(특징과 기계학습 방식)에서는 특징을 사람이 설계했지만, 신경망은 이미지에 포함된 중요한 특징까지도 '기계'가 스스로 학습할 것이다.**\n",
    "\n",
    "**딥러닝을 종단간 기계학습(end-to-end machine learning)이라고도 한다. 여기서 종단간은 '처음부터 끝까지'라는 의미로, 데이터(입력)에서 목표한 결과(출력)를 사람의 개입 없이 얻는다는 뜻을 담고 있다.**\n",
    "\n",
    "> **신경망의 이점은 모든 문제를 같은 맥락에서 풀 수 있다는 점에 있다.**\n",
    "\n",
    "**즉, 신경망은 모든 문제를 주어진 데이터 그대로를 입력 데이터로 활용해 'end-to-end'로 학습할 수 있다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1.2 훈련 데이터와 시험 데이터**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**기계학습 문제는 데이터를 훈련 데이터(training data)와 시험 데이터(test data)로 나눠 학습과 실험을 수행하는 것이 일반적이다.**\n",
    "\n",
    "> **우선 훈련 데이터만 사용하여 학습하면서 최적의 매개변수를 찾는다. 그런 다음 시험 데이터를 사용하여 앞서 훈련한 모델의 실력을 평가한다.**\n",
    "\n",
    "**우리가 원하는 것은 범용적으로 사용할 수 있는 모델이기 때문이다. 이 범용 능력을 제대로 평가하기 위해 훈련 데이터와 시험 데이터를 분리한다.**\n",
    "\n",
    "> **참고로 한 데이터셋에만 지나치게 최적화된 상태를 오버피팅(overfiting)이라고 한다. 오버피팅 피하기는 기계학습의 중요한 과제이기도 한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2 손실 함수**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**신경망 학습에서는 현재의 상태를 '하나의 지표'로 표현한다. 그리고 그 지표를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색하는 것이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**신경망도 '하나의 지표'를 기준으로 최적의 매개변수 값을 탐색한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**신경망 학습에서 사용하는 지표는 손실 함수(loss function)라고 한다. 이 손실 함수는 임의의 함수를 사용할 수도 있지만 일반적으로는 평균 제곱 오차와 교차 엔트로피 오차를 사용한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2.1 평균 제곱 오차**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**가장 많이 쓰이는 손실 함수는 평균 제곱 오차(mean squared error, MSE)이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](./img/평균제곱오차.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**여기서는 y는 신경망의 출력 (신경망이 추정한 값), t는 정답 레이블, k는 데이터의 차원 수를 나타낸다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_squared_error(y,t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09750000000000003\n",
      "0.6975\n"
     ]
    }
   ],
   "source": [
    "y = [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0] # 2번째 index로 예측\n",
    "y2 = [0.1,0.05,0.0,0.0,0.05,0.1,0.6,0.1,0.0,0.0] # 6번째 index로 예측\n",
    "t = [0,0,1,0,0,0,0,0,0,0] # 2 번째 index\n",
    "print(mean_squared_error(np.array(y),np.array(t))) # MSE = 0.0975\n",
    "print(mean_squared_error(np.array(y2),np.array(t))) # MSE = 0.6975"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2.2 교차 엔트로피 오차**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**또 다른 손실 함수로서 교차 엔트로피 오차(cross entropy error, CEE)도 자주 사용한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](./img/교차엔트로피오차.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**여기에서 log는 밑이 e인 자연로그이다. y는 신경망의 출력, t는 정답 레이블이다.**\n",
    "\n",
    "> **따라서 실질적으로 정답일 때의 추정 (t가 1일 때의 y)의 자연로그를 계산하는 식이 된다.**\n",
    "\n",
    "**즉, 교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정하게 된다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](./img/교차엔트로피오차1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7 # np.log에 0이 들어가면 무한대로 값이 커지므로 아주 작은 값을 더해준다.\n",
    "    return -np.sum(t*np.log(y+delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n",
      "16.11809565095832\n"
     ]
    }
   ],
   "source": [
    "y = [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0] # 2번째 index로 예측\n",
    "y2 = [0.1,0.05,0.0,0.0,0.05,0.1,0.6,0.1,0.0,0.0] # 6번째 index로 예측\n",
    "t = [0,0,1,0,0,0,0,0,0,0] # 2번째 index\n",
    "\n",
    "print(cross_entropy_error(np.array(y),np.array(t))) # -log0.6 = 0.5108..\n",
    "print(cross_entropy_error(np.array(y2),np.array(t))) # -log0.0 = 16.11.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2.3 미니배치 학습**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**기계학습 문제는 훈련 데이터를 사용해 학습한다. 더 구체적으로 말하면 훈련 데이터에 대한 손실 함수의 값을 구하고, 그 값을 최대한 줄여주는 매개변수를 찾아낸다.**\n",
    "\n",
    "> **이렇게 하려면 모든 훈련 데이터를 대상으로 손실 함수 값을 구해야 한다. 즉, 훈련 데이터가 100개 있으면 그로부터 계산한 100개의 손실 함수 값들의 합을 지표로 삼는 것이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![test](./img/엔트로피.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**이때 데이터가 N개라면 t는 n번째 데이터의 k번째 값을 의미한다.(y는 신경망의 출력, t는 정답 레이블이다.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**그리고 마지막에 N으로 나누어 정규화를 진행한다.(N으로 나눔으로써 '평균 손실 함수'를 구하는 것이다.)**\n",
    "\n",
    "> **이렇게 평균을 구해 사용하면 훈련 데이터 개수와 관계없이 언제든 통일된 지표를 얻을 수 있다.**\n",
    "\n",
    "**많은 양의 빅데이터를 대상으로 손실 함수의 합을 구하려면 많은 시간이 걸리기 때문에 이런 경우 데이터의 일부를 추려 전체의 '근사치'로 이용한다.**\n",
    "\n",
    "> **신경망 학습에서 훈련 데이터로 부터 일부만 골라서 학습 하는 것을 미니배치 학습(mini-batch)라 한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train), (X_test,y_test) = load_mnist(normalize=True,one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**앞의 코드에서 MNIST 데이터를 읽은 결과, 훈련 데이터는 60,000개고, 입력 데이터는 784열 (원래는 28 x 28)인 이미지 데이터임을 알 수 있다. 또, 정답 레이블은 10줄짜리 데이터이다. 그래서 잎의 x_train, y_train의 모습은 각각 (60000,784)와 (60000,10)이 된다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**그러면 이 훈련 데이터에서 무작위로 10장만 빼내려면 어떻게 하면 될까? 넘파이의 np.random.choice() 함수를 쓰면 다음과 같이 해결가능하다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size, batch_size 설정\n",
    "train_size = X_train.shape[0] # 60,000\n",
    "batch_size = 10 # batch 수 \n",
    "\n",
    "# np.random.choice: batch_size 랜덤하게 배치사이즈만큼의 랜덤 값들을 추출하여 batch_mask에 할당\n",
    "# np.random.choice(train_size,batch_size) train_size 미만의 수 중에서 batch_size만큼 무작위로 골라낸다.\n",
    "batch_mask = np.random.choice(train_size,batch_size) \n",
    "\n",
    "# batch 처리\n",
    "X_batch = X_train[batch_mask]\n",
    "y_batch = y_train[batch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2.4 (배치용) 교차 엔트로피 오차 구현하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**조금 전 구현한 교차 엔트로피 오차(데이터를 하나씩 처리하는 구현)을 조금만 바꿔주면 된다. 여기에서는 데이터가 하나인 경우와 데이터가 배치로 묶여 입력될 겨우 모두를 처리할 수 있도록 구현한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    if y.dim == 1: # y가 1차원이라면,\n",
    "        t = t.reshape(1,t.size) \n",
    "        # 데이터 하나당 교차 엔트로피 오차를 구하는 경우 데이터의 형상을 바꿔준다. \n",
    "        y = y.reshape(1,y.size) \n",
    "    batch_size = y.shape[0] # 훈련데이터의 행 데이터의 수를 batch_size로 지정\n",
    "    return -np.sum(t*np.log(y+1e-7)) / batchsize # 엔트로피 오차의 평균을 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**이 코드에서 y는 신경망의 출력, t는 정답 레이블이다. y가 1차원이라면, 즉 데이터 하나당 교차 엔트로피 오차를 구하는 경우는 reshape 함수로 데이터의 형상을 바꿔준다. 그리고 배치의 크기로 나눠 정규화하고 이미지 1장당 평균의 교차 엔트로피 오차를 계산한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**정답 레이블이 원-핫 인코딩이 아니라 '2'나 '7' 등의 숫자 레이블로 주어졌을 때의 교차 엔트로피 오차는 다음과 같이 구현할 수 있다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error_no_ont_hot(y,t):\n",
    "    if y.dim == 1:\n",
    "        t = t.reshape(1,t.size)\n",
    "        y = y.reshape(1,y_size)\n",
    "        \n",
    "    batch_size = y.shape[0] \n",
    "    return -np.sum(np.log(y[np.arange(batch_size),t]+1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**np.log(y[np.arange(batch_size),t])**\n",
    "\n",
    "> **np.arange 0 ~ batch_size - 1 까지의 배열 생성**\n",
    "\n",
    "> **t에는 실제 레이블이 저장되어있으므로 예를들어 [2,7,0,9,4]와 같이 저장**\n",
    "\n",
    "> **즉, y[0,2],y[1,7],y[2,0],.., 같이 저장**\n",
    "\n",
    "**이 구현의 핵심은 정답에 해당하는 신경망의 출력만으로 교차 엔트로피 오차를 계산한다는 점이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2.5 왜 손실 함수를 설정하는가?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**'정확도'라는 지표를 놔두고 '손실 함수의 값'이라는 우회적인 방법을 택하는 이유는 신경망 학습에서의 '미분'의 역할에 주목해야한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**신경망 학습에서는 최적의 매개변수(가중치와 편향)를 탐색할 때 손실 함수의 값을 가능한 작게 하는 매개변수 값을 찾는다. 이때 매개변수의 미분(정확히는 기울기)을 계산하고, 그 미분 값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**가중치 매개변수의 손실 함수의 미분이란 '가중치 매개변수의 값을 아주 조금 변화 시켰을 때, 손실 함수가 어떻게 변하나'라는 의미이다. 만약 이 미분 값이 음수면 그 가중치 매개변수를 양의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있다. 반대로, 이 미분 값이 양수면 그 가중치 매개변수를 음의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있다.**\n",
    "\n",
    "> **그러나 미분 값이 0이면 가중치 매개변수를 어느 쪽으로 움직여도 손실 함수의 값은 달라지지 않는다. 그래서 그 가중치 매개변수의 갱신은 거기서 멈춘다.**\n",
    "\n",
    "**신경망을 학습할 때 정확도를 지표로 삼아서는 안 된다. 정확도를 지표로 하면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문이다.**\n",
    "\n",
    "> **정확도는 미세한 변화에는 거의 반응을 보이지 않고, 반응이 있더라도 그 값이 불연속적으로 갑자기 변화한다. 이는 '계단 함수'를 활성화 함수로 사용하지 않는 이유와도 들어맞다.**\n",
    "\n",
    "![test](./img/계단함수와시그모이드함.png)\n",
    "\n",
    "**계단 함수는 한순간만 변화를 일으키지만, 시그모이드 함수의 미분(접선)은 출력이 연속적으로 변하고 곤선의 기울기도 연속적으로 변한다. 즉, 시그모이드 함수의 미분은 어느 장소라도 0이 되지 않는다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**이는 신경망 학습에서 중요한 성질로, 기울기가 0이 되지 않는 덕분에 신경망이 올바르게 학습할 수 있다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3 수치 미분**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**경사법에서는 기울기(경사) 값을 기준으로 나아갈 방향을 정한다. 기울기란 무엇인지, 또 어떤 성질이 있는지 설명하기 앞서, 이번 절에서는 학생 때 배운 '미분'부터 복습한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.3.1 미분**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10분에 2km씩 달렸다고 가정하면, 이때의 속도는 간단히 2/10 = 0.2[km/분]이라고 계산할 수 있다. 즉, 1분에 0.2km만큼의 속도(변화)로 뛰었다고 해석할 수 있다.**\n",
    "\n",
    "> **다만 여기에서 10분에 2km를 뛰었다는 것은, 정확하게는 10분 동안의 '평균 속도'를 구한 것이다.**\n",
    "\n",
    "**미분은 '특정 순간'의 변화량을 뜻한다. 그래서 10분이라는 시간을 가능한 한 줄여(직전 1분에 달린 거리, 직전 1초에 달린 거리, 직전 0.1초에 달린 거리... 식으로 갈수록 간격을 줄여) 한 순간의 변화량(어느 순간의 속도)을 얻는 것이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](./img/미분1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**이처럼 미분은 한순간의 변화량을 표시한 것이다. 좌변 f(x)의 x에 대한 미분(x에 대한 f(x)의 변화량)을 나타내는 기호이다. 결국, x의 '작은 변화'가 함수 f(x)를 얼마나 변화시키느냐를 의미한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**이때 시간의 작은 변화, 즉 시간을 뜻하는 h를 한없이 0에 가깝게 한다는 의미를 lim로 나타낸다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f의 차분(임의의 두 점에서의 함수 값들의 차이) (x+h)와 x 사이의 함수 f의 차분을 계산하고 있지만, 애당초 이 계산에는 오차가 있다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**'진정한 미분'은 x 위치의 함수의 기울기(이를 접선이라 함)에 해당하지만, 위 식으로 보면 (x+h)와 x사이에 기울기에 해당하므로 진정한 미분(진정한 접선)과 이번 구현의 값은 엄밀히는 일치하지 않는다. 이 차이는 h를 무한히 0으로 좁히는 것이 불가능해 생기는 한계이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](./img/접선기울기.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**위 그림과 같이 수치 미분에는 오차가 포함된다. 이 오차를 줄이기 위해 (x+h)와 (x-h)일 때의 함수 f의 차분을 계산하는 방법을 쓰기도 한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**이 차분은 x를 중심으로 그 전후의 차분을 계산한다는 의미에서 중심 차분 혹은 중앙 차분이라 한다.(한편, (x+h)와 x의 차분은 전방 차분이라 한다.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f,x): # f 함수 / x 인수\n",
    "    h = 1e-4 # 분모가 0이 되는 것을 방지\n",
    "    return (f(x+h)-f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = 0.01x^2 + 0.1x\n",
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0.0,20.0,0.1) # 0 ~ 20에서 0.1 간격으로 원소 생성\n",
    "y = function_1(x)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n",
      "0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "print(numerical_diff(function_1,5)) # X = 5 일때 미분(접선의 기울기)값\n",
    "print(numerical_diff(function_1,10)) # X = 10 일때 미분(접선의 기울기)값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tangent_line(f, x):\n",
    "    d = numerical_diff(f, x)\n",
    "    print(d)\n",
    "    y = f(x) - d*x\n",
    "    return lambda t: d*t + y\n",
    "     \n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "\n",
    "tf = tangent_line(function_1, 5)\n",
    "y2 = tf(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, y2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.3.3 편미분**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**앞의 예와 달리 변수가 2개라는 점에 주의해야 한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](./img/편미분.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](./img/편미분그래.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**변수가 두개일 때 미분의 주의할 점은 변수가 2개라는 점이다. 그래서 '어느 변수에 대한 미분이냐', 즉 x0와 x1 중 어는 변수에 대한 미분이냐를 구별해야 한다. 덧붙여 이와 같이 변수가 여럿인 함수에 대한 미분을 편미분이라 한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x0 = 3 , x1 = 4 일 때, x0에 대한 편미분\n",
    "def function_tmp1(x0):\n",
    "    return x0**2 + 4.0 ** 2.0\n",
    "\n",
    "# x0 = 3, x1 = 4 일 때, x1에 대한 편미분\n",
    "def function_tmp2(x1):\n",
    "    return 3.0 ** 2.0 + x1 ** 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.00000000000378\n",
      "7.999999999999119\n"
     ]
    }
   ],
   "source": [
    "print(numerical_diff(function_tmp1,3.0))\n",
    "print(numerical_diff(function_tmp2,4.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**이처럼 편미분은 변수가 하나인 미분과 마찬가지로 특정 장소의 기울기를 구한다. 단, 여러 변수 중 목표 변수 하나에 초점을 맞추고 다른 변수는 값을 고정한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.4 기울기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**x0와 x1의 편미분을 동시에 계산하고 싶다면 어떻게 할까? 가령 x0 = 3, x1 = 4일 때 (x0,x1) 양쪽의 편미분을 묶어서 계산한다고 생각할때, 이때 모든 변수의 편미분을 벡터로 정리한 것을 기울기(Gradient)라고 한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f,x):\n",
    "    h = 1e-4 # 분모가 0이 되는걸 방지\n",
    "    grad = np.ones_like(x) # x와 형상이 같은 0을 원소로 가진 배열 생성\n",
    "    \n",
    "    for idx in range(x.size): # x의 원소의 개수만큼 반복\n",
    "        tmp_val = x[idx] # 원래의 원소를 tmp_val에 임시 저장\n",
    "        \n",
    "        # f(x+h)\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x) # x[0]**2 + x[1]**2 -> 여기선 x[0]만 미세하게 바뀜\n",
    "        \n",
    "        # f(x-h)\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x) # x[0]**2 + x[1]**2 -> 여기선 x[1]만 미세하게 바뀜\n",
    "        \n",
    "        # f(x+h) - f(x-h) / 2*h\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 원래의 값으로 되돌림\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[0. 4.]\n",
      "[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(numerical_gradient(function_2,np.array([3.0,4.0])))\n",
    "print(numerical_gradient(function_2,np.array([0.0,2.0])))\n",
    "print(numerical_gradient(function_2,np.array([3.0,0.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](./img/기울기.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**그런데 이 기울기라는 게 의미하는 건 무엇인지는 위 그림을 보면 알 수 있다. 기울기 그림은 위 그림 처럼 방향을 가진 벡터(화살표)로 그려진다. 이 그림을 보면 기울기는 함수의 '가장 낮은 장소(최솟값)'를 가르킨다. 마치 나침반처럼 화살표들은 한 점을 향하고 있다. 또 '가장 낮은 곳'에서 멀어질수록 화살표의 크기가 커짐을 알 수 있다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**기울기는 가장 낮은 장소를 가리키지만, 사실 기울기는 각 지점에서 낮아지는 방향을 가리킨다. 더 정확히 말하자면 기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.4.1 경사법(경사 하강법)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**기계학습 문제 대부분은 학습 단계에서 최적의 매개변수를 찾아낸다. 신경망 역시 최적의 매개변수(가중치와 편향)를 학습 시에 찾아야 한다.**\n",
    "\n",
    "> **여기에서 최적이란 손실 함수가 최솟값이 될 때의 매개변수 값이다. 그러나 일반적인 문제의 손실 함수는 매우 복잡하다. 매개변수 공간이 광대하여 어디가 최솟값이 되는 곳인지를 짐작할 수 없다.**\n",
    "\n",
    "**이런 상황에서 기울기를 잘 이용해 함수의 최솟값(또는 가능한 한 작은 값)을 찾으려는 것이 경사법이다. 여기에서 주의할 점은 각 지점에서 함수의 값을 낮추는 방안을 제시하는 지표가 기울기이다.**\n",
    "\n",
    "> **그러나 기울기가 가리키는 곳에 정말 함수의 최솟값이 있는지, 즉 그쪽이 정말로 나아가야 할 방향인지는 보장할 수 없다. 실제로 복잡한 함수에서는 기울기가 가리키는 방향에 최솟값이 없는 경우가 대부분이다.**\n",
    "\n",
    "**기울어진 방향이 꼭 최솟값을 가리키는 것은 아니나, 그 방향으로 가야 함수의 값을 줄일 수 있다. 그래서 최솟값이 되는 장소를 찾는 문제에서는 기울기 정보를 단서로 나아갈 방향을 정한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한다. 그런 다음 이동한 곳에서도 마찬가지로 기울기를 구하고, 또 그 기울어진 방향으로 나아가기를 반복한다. 이렇게 해서 함수의 값을 점차 줄이는 것이 경사법(Gradient method)이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](./img/경사법.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**η기호(eta,에타)는 갱신하는 양을 나타낸다. 이를 신경망 학습에서는 학습률(learning rate)라고 한다. 한 번의 학습으로 얼마만큼 학습해야 할지, 즉 매개변수 값을 얼마나 갱신하느냐를 정하는 것이 학습률이다.**\n",
    "\n",
    "> **여기에서는 변수가 2개인 경우를 보여줬지만, 변수의 수가 늘어도 같은 식(각 변수의 편미분 값)으로 갱신하게 된다.**\n",
    "\n",
    "**또한 학습률 값은 0.01이나 0.001 등 미리 특정 값으로 정해두어야 하는데, 일반적으로 이 값이 너무 크거나 작으면 '좋은 장소'를 찾아갈 수 없다.**\n",
    "\n",
    "> **신경망 학습에서는 보통 이 학습률 값을 변경하면서 올바르게 학습하고 있는지를 확인하면서 진행한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f,init_x,lr=0.01,step_num=100): \n",
    "    # f: 최적화하려는 함수, init_x: 초기값(weight), lr은 learning rate, step_num: 반복 수\n",
    "    x = init_x # 초기 weight\n",
    "    \n",
    "    for i in range(step_num): # 학습을 100번 진행\n",
    "        grad = numerical_gradient(f,x) # 각각을 편미분 하여 매 epoch별 weight의 기울기를 구함\n",
    "        x -= lr * grad # x(weight) -= lr(학습률) * grad(기울기)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에서는 초깃값을 (-3.0,4.0)으로 설정한 후 경사법을 사용해 최솟값 탐색을 시작한다.\n",
    "# 최종 결과는 (-6.1e-10,8.1e-10)으로, 거의 (0,0)에 가까운 결과이다. 실제로 진정한 최솟값은 (0,0)이므로 경사법으로 거의 정확한 결과를 얻었다.\n",
    "init_x = np.array([-3.0,4.0]) # weight\n",
    "gradient_descent(function_2,init_x=init_x,lr=0.1,step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](./img/등고선.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n"
     ]
    }
   ],
   "source": [
    "# 학습률이 너무 크면 큰 값으로 발산해버리고, 반대로 너무 작으면 거의 갱신되지 않은 채 끝나버린다.\n",
    "\n",
    "# 학습률이 너무 큰 예\n",
    "init_x = np.array([-3.0,4.0])\n",
    "print(gradient_descent(function_2,init_x=init_x, lr=10.0, step_num=100))\n",
    "\n",
    "# 학습률이 너무 작은 예\n",
    "print(gradient_descent(function_2,init_x=init_x, lr=1e-10, step_num=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**학습률 같은 매개변수를 하이퍼파라미터(hyper parameter)라고 한다. 이는 가중치와 편향 같은 신경망의 매개변수와는 성질이 다른 매개변수이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**신경망의 가중치 매개변수는 훈련 데이터와 학습 알고리즘에 의해서 '자동'으로 획득되는 매개변수인 반면, 학습률 같은 하이퍼파라미터는 사람이 직접 설정해야 하는 매개변수이다.**\n",
    "\n",
    "> **일반적으로 이들 하이퍼파라미터는 여러 후보 값 중에서 시험을 통해 가장 잘 학습하는 값을 찾는 과정을 거쳐야한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.4.2 신경망에서의 기울기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**신경망 학습에서도 기울기를 구해야 한다. 여기서 말하는 기울기는 가중치 매개변수에 대한 손실 함수의 기울기이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**여기서 중요한 점은 W의 편미분한 형상과 W의 형상이 같다는 것이다.(둘다 2x3 이다.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **신경망에 필요 함수들 정리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x)) # np.exp(-x) = exp(-x)\n",
    "\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c) # Overflow 방지\n",
    "    # Softmax formula\n",
    "    sum_exp_a = np.sum(exp_a) \n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7 # log0이 되는 것을 방지\n",
    "    # cross_entropy formula\n",
    "    return -np.sum(t*np.log(y+delta))\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 분모가 0이 되는 것을 방지\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        \n",
    "        # differential\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SimpleNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # normal_distbution random seed\n",
    "    def predict(self,x):\n",
    "        return np.dot(x,self.W) # Hypothesis\n",
    "    def loss(self,x,t):\n",
    "        z = self.predict(x) # Hypothesis\n",
    "        y = softmax(z) # Hypothesis의 결과 값을 softmax 함수를 통해 확률값으로 변환\n",
    "        loss = cross_entropy_error(y,t) # Cost(cross_entropy)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [0.6 0.9]\n",
      "t: [0 0 1]\n",
      "init Weight:\n",
      " [[-0.84703907 -0.42695282  0.06578689]\n",
      " [ 0.05469507 -1.15864863  1.10683186]]\n",
      "Hypothesis: [-0.45899788 -1.29895546  1.03562081]\n",
      "argmax: 1\n",
      "Cost: 0.27852941653164626\n"
     ]
    }
   ],
   "source": [
    "# SimpleNet class \n",
    "net = simpleNet()\n",
    "\n",
    "# dataset\n",
    "x = np.array([0.6,0.9]) # feature\n",
    "t = np.array([0,0,1]) # label\n",
    "print('X:',x)\n",
    "print('t:',t)\n",
    "\n",
    "# random weight output\n",
    "print('init Weight:\\n',net.W) \n",
    "\n",
    "# Hypothesis\n",
    "p = net.predict(x) \n",
    "print('Hypothesis:',p) \n",
    "\n",
    "# Max index \n",
    "print(\"argmax:\",np.argmax(x))\n",
    "\n",
    "# Cost function\n",
    "cost = net.loss(x,t)\n",
    "print(\"Cost:\",cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10187854  0.0439839  -0.14586244]\n",
      " [ 0.15281781  0.06597585 -0.21879366]]\n"
     ]
    }
   ],
   "source": [
    "# gradient 내부에서 f(x)를 실행하는데, 그와 일관성을 주기 위한 함수 f\n",
    "# f = lambda w: net.loss(x,t) \n",
    "def f(W): \n",
    "    return net.loss(x,t) \n",
    "\n",
    "# 각각의 loss 값일때 Gradient를 구한다.\n",
    "dW = numerical_gradient(f,net.W) \n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**결과를 보면 1행 1열의 값이 대략 0.17인데 이는 W11을 h만큼 늘리면 손실 함수의 값은 0.17h 만큼 증가한다는 의미다. 마찬가지로 2행 3열은 -0.84로 W23을 h만큼 늘리면 손실 함수의 값은 0.84만큼 감소하는 것이다. 그래서 손실 함수를 줄인다는 관점에서는 W23은 양의 방향으로 W11은 음의 방향으로 갱신해야 함을 알 수 있다. 또한, W23이 W11 보다 크게 기여한다는 사실도 알 수 있다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.5 학습 알고리즘 구현하기** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**신경망 학습의 절차는 다음과 같다.**\n",
    "\n",
    "> **전제: 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다. 신경망 학습은 다음과 같다.**\n",
    "\n",
    "> **1단계-미니배치: 훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표이다.**\n",
    "\n",
    "> **2단계-기울기 산출: 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.**\n",
    "\n",
    "> **3단계-매개변수 갱신: 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.**\n",
    "\n",
    "> **4단계-반복: 1 ~ 3단계를 반복한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self,input_size,hidden_size,output_size,weight_init_std=0.01):\n",
    "        # __init__ 클래스를 초기화(초기화 메서드는 클래스를 생성할 때 불리는 메서드)\n",
    "        # input_size: 입력층 노드 설정\n",
    "        # hidden_size: 은닉층 노드 설정\n",
    "        # output_size: 출력층 노드 설정\n",
    "        # weight_init_std: learning rate\n",
    "        # \n",
    "        self.params = {} # 빈 딕셔너리 생성\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size,hidden_size) # Learning rate * W1\n",
    "        self.params['b1'] = np.zeros(hidden_size) # np.zeros 은닉층 노드의 개수만큼 1차원 원소 0을 생성\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size,output_size) # learning rate * W2\n",
    "        self.params['b2'] = np.zeros(output_size) # np.zeros 출력층 노드의 개수만큼 1차원 원소 0을 생성\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # 가중치 할당 딕셔너리의 키 값을 통해 각각의 value 값을 W1,W2,b1,b2에 할당\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        # Hypothesis\n",
    "        a1 = np.dot(x,W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1,W2) + b2\n",
    "        y = softmax(a2)       \n",
    "        return y\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        # loss (cross_entropy)\n",
    "        y = self.predict(x)       \n",
    "        return cross_entropy_error(y,t)\n",
    "    \n",
    "    def accuracy(self,x,t):\n",
    "        # Hypothesis\n",
    "        y = self.predict(x)\n",
    "        # np.argmax를 통해 각 row의 가장 높은 값의 인덱스를 추출\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        # accuracy\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0]) \n",
    "        return accuracy\n",
    "    def numerical_gradient(self,x,t):\n",
    "        # loss 값을 loss_W에 할당\n",
    "        loss_W = lambda W: self.loss(x,t)\n",
    "        # 각각의 gradient를 담을 공간 \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W,self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W,self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W,self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W,self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# TwoLayerNet 클래스는 딕셔너리인 params와 grads를 인스턴스 변수로 갖는다.\n",
    "# 그리고 params 변수에 저장된 가중치 매개변수가 예측 처리(순방향 처리)에서 사용된다.\n",
    "net = TwoLayerNet(input_size=784, hidden_size=100,output_size=10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 784)\n",
      "(100, 10)\n",
      "(100, 10)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100,784) # x_data 100,784 즉, 100개의 데이터 784개의 특성을 지님\n",
    "y = net.predict(x)\n",
    "t = np.random.rand(100,10)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# grads 변수에는 params 변수에 대응하는 각 매개변수의 기울기가 저장된다.\n",
    "grads = net.numerical_gradient(x,t)\n",
    "\n",
    "# 각 매개변수의 기울기\n",
    "print(grads['W1'].shape)\n",
    "print(grads['b1'].shape)\n",
    "print(grads['W2'].shape)\n",
    "print(grads['b2'].shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**__init__(self,input_size,hidden_size,output_size) 메서드는 클래스를 초기화한다.**\n",
    "\n",
    "> **입력층, 은닉층, 출력층을 설정하고, 가중치 매개변수도 초기화한다.**\n",
    "\n",
    "**가중치 매개변수의 초깃값을 무엇으로 설정하냐가 신경망 학습의 성공을 좌우한다.**\n",
    "\n",
    "> **하지만 당장은 정규분포를 따르는 난수로, 편향은 0으로 초기화한다고 이야기하고 넘어간다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**numerical_gradient(self,x,t)는 수치 미분 방식으로 매개변수의 기울기를 계산한다. 다음 장에서는 이 기울기 계산을 고속으로 수행하는 기법을 설명하는데, 그 방법은 바로 오차역전파법이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**오차역전파법을 쓰면 수치 미분을 사용할 때와 거의 같은 결과를 훨씬 빠르게 얻을 수 있다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.5.2 미니배치 학습 구현하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**미니배치 학습이란 훈련 데이터 중 일부를 무작위로 꺼내고(미니배치), 그 미니배치에 대해서 경사법으로 매개변수를 갱신한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# data import\n",
    "(x_train,y_train), (x_test,y_test) = load_mnist(normalize=True,one_hot_label=True)\n",
    "\n",
    "train_loss_list = [] # loss list\n",
    "\n",
    "# hyper parameter\n",
    "iters_num = 10 # iteration\n",
    "train_size = x_train.shape[0] # train_size\n",
    "batch_size = 100 # mini batch size\n",
    "learning_rate = 0.1 # learning rate\n",
    "\n",
    "# network\n",
    "network = TwoLayerNet(input_size=784,hidden_size=50,output_size=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "for i in range(iters_num): \n",
    "    # train_size 중에서 batch_size만큼 랜덤으로 인덱스 위치를 추출\n",
    "    batch_mask = np.random.choice(train_size,batch_size) \n",
    "    x_batch = x_train[batch_mask] # 인덱스 번호에 있는 데이터 할당\n",
    "    t_batch = y_train[batch_mask] # 인덱스 번호에 있는 데이터 할당\n",
    "    \n",
    "    # Gradient\n",
    "    grad = network.numerical_gradient(x_batch,t_batch) \n",
    "    \n",
    "    # parameter update\n",
    "    for key in ('W1','b1','W2','b2'): \n",
    "        network.params[key] -= learning_rate * grad[key] # (W,b) -= learning rate * gradient\n",
    "    \n",
    "    # cost or loss function\n",
    "    loss = network.loss(x_batch,t_batch) \n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**학습 횟수가 늘어가면서 손힐 함수의 값이 줄어든다. 이는 학습이 잘 되고 있다는 뜻으로, 신경망의 가중치 매개변수가 서서히 데이터에 적응하고 있음을 의미한다. 바로 신경망이 학습하고 있는 것이다. 다시 말해 데이터를 반복해서 학습함으로써 최적 가중치 매개변수로 서서히 다가가고 있다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.5.3 시험 데이터로 평가하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**미니배치 학습 과정에서 손실 함수의 값이란, 정확히는 '훈련 데이터의 미니배치에 대한 손실함수'의 값이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**따라서 이 결과만으로는 다른 데이터셋에도 비슷한 실력을 발휘할지는 확실하지 않다.**\n",
    "\n",
    "> **다른 말로 '오버피팅'을 일으키지 않는지 확인해야한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**신경망 학습의 목표는 범용적인 능력을 익히는 것이다. 이를 위해 다음 구현에서는 학습 도중 정기적으로 훈련 데이터와 시험 데이터를 대상으로 정확도를 기록한다.**\n",
    "\n",
    "> **에폭(epoch)은 하나의 단위이다. 1에폭은 학습에서 훈련 데이터를 모두 소진했을 때의 횟수에 해당 예컨대 훈련 데이터 10,000개를 100개의 미니배치로 학습할 경우, 확률적 경사하강법을 100회 반복하면 모든 훈련 데이터를 '소진'하게 된다. 이 경우 100회가 1에폭이 된다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# data import\n",
    "(x_train,y_train), (x_test,y_test) = load_mnist(normalize=True,one_hot_label=True)\n",
    "\n",
    "train_loss_list = [] # loss list\n",
    "train_acc_list = [] # train accuracy list\n",
    "test_acc_list = [] # test accuracy list\n",
    "\n",
    "# hyper parameter\n",
    "iters_num = 10 # iteration 10000\n",
    "train_size = x_train.shape[0] # train_size\n",
    "batch_size = 100 # mini batch size\n",
    "learning_rate = 0.1 # learning rate\n",
    "\n",
    "# 1epoch iter\n",
    "iter_per_epoch = (train_size/batch_size)\n",
    "\n",
    "# network\n",
    "network = TwoLayerNet(input_size=784,hidden_size=50,output_size=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.11236666666666667,0.1135\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for i in range(iters_num): \n",
    "    # train_size 중에서 batch_size만큼 랜덤으로 인덱스 위치를 추출\n",
    "    batch_mask = np.random.choice(train_size,batch_size) \n",
    "    x_batch = x_train[batch_mask] # 인덱스 번호에 있는 데이터 할당\n",
    "    t_batch = y_train[batch_mask] # 인덱스 번호에 있는 데이터 할당\n",
    "    \n",
    "    # Gradient\n",
    "    grad = network.numerical_gradient(x_batch,t_batch) \n",
    "    \n",
    "    # parameter update\n",
    "    for key in ('W1','b1','W2','b2'): \n",
    "        network.params[key] -= learning_rate * grad[key] # (W,b) -= learning rate * gradient\n",
    "    \n",
    "    # cost or loss function\n",
    "    loss = network.loss(x_batch,t_batch) \n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1epoch accuracy\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train,y_train)\n",
    "        test_acc = network.accuracy(x_test, y_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc |\" + str(train_acc) + ',' + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](./img/미니배치.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**이 예에서는 1에폭마다 모든 훈련 데이터와 시험 데이터에 대한 정확도를 계산하고, 그 결과를 기록한다.**\n",
    "\n",
    "> **정확도를 1에폭마다 계산하는 이유는 for 문 안에서 매번 계산하기에는 시간이 오래 걸리고, 또 그렇게까지 자주 기록할 필요도 없기 때문이다. 더 큰 관점에서 그 추이를 알 수 있으면 충분하다.**\n",
    "\n",
    "**보다시피 에폭이 진행될수록(학습이 진행될수록) 훈련 데이터와 시험 데이터를 사용하고 평가한 정확도가 모두 좋아지고 있다. 또, 두 정확도에는 차이가 없음을 알 수 있다.(두 선이 거의 겹쳐있다.) 다시 말해 이번 학습에서 오버피팅이 일어나지 않았다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = x_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
