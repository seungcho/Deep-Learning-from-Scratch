{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4장 신경망 학습\n",
    "- 학습이란 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것을 뜻한다.\n",
    "- 신경망이 학습할 수 있도록 해주는 지표인 __손실함수__는 결괏값을 가장 작게 만드는 가중치 매개변수를 찾는 것이 학습의 목표이다.\n",
    "\n",
    "## 1.1 데이터에서 학습\n",
    "- 신경망의 특징은 데이터를 보고 학습할 수 있다는 점이다.\n",
    "- 데이터에서 학습한다는 것은 __가중치 매개변수의 값을 데이터를 보고 자동으로 결정__한다는 뜻이다.\n",
    "\n",
    "## 1.2 데이터 주도 학습\n",
    "- 기계학습은 __데이터가 생명__이다.\n",
    "- 데이터에서 답을 찾고 데이터에서 패턴을 발견하고 데이터로 이야기를 만드는, 그것이 바로 기계학습이다.\n",
    "> - 보통 패던을 찾을때 사람이 이것저것 생각하고 답을 찾는 것이 일반적이지만 사람의 개입이 들어가기마련이다. 하지만 기계학습에서는 사람의 개입을 최소화하고 수집한 데이터로부터 패턴을 찾으려 시도한다.\n",
    "- 기계학습에서 모아진 데이터로부터 규칙을 찾아내는 역할을 __기계__가 담당한다.\n",
    "> - 하지만 데이터의 특징을 벡터화 할때 여전히 __사람__이 설계하는 것임을 주의해야한다.\n",
    "> - 즉, 설계를 잘못하면 기계학습을 해도 좋은 결과를 얻지 못한다는 점이다.\n",
    "- 반면 신경망은 있는 그대로를 학습한다.\n",
    "> - 즉, MNIST를 예로 하면 기존의 기계학습은 이미지 데이터를 설계할때 여전히 사람의 개입이 들어가지만 신경망을 이미지에 포함된 중요한 특징까지도 __기계__가 스스로 학습한다.\n",
    "- 신경망의 이점은 모든 문제를 같은 맥락에서 풀 수 있다는 점이다.\n",
    "> - 예를 들어 '5'를 인식하는 문제든, '개'를 인식하는 문제든 '사람얼굴'을 인식하는 문제든 세부사항과 관계없이 신경망은 주어진 데이터를 온전히 학습하고, 주어진 문제의 패턴을 발견하려고 시도한다.\n",
    "\n",
    "## 1.3 훈련 데이터와 시험 데이터\n",
    "- 기계학습 문제는 데이터를 훈련 데이터와 시험 데이터로 나눠 학습과 실험을 수행하는 것이 일반적이다.\n",
    "> - 아직 보지 못한 데이터로도 문제를 올바르게 풀어내기 위해 훈련 데이터로 데이터를 학습 시키고 시험 데이터를 통해 보지 못한 데이터가 들어와도 올바르게 예측하는지를 확인한다.\n",
    "- 따라서 데이터셋에만 지나치게 최적화된 상태 즉, 오버피팅을 피하기는 기계학습에서 중요한 과제이다.\n",
    "\n",
    "## 2.1손실함수\n",
    "- 신경망 학습에서는 현재의 상태를 하나의 지표로 표현하는데 그 지표를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색하는 것이 목표이다. 그 지표를 손실 함수(loss function)이라 한다.\n",
    "> - 일반적으로 이 손실함수는 평균 제곱 오차와 교차 엔트로피 오차를 사용한다.\n",
    "\n",
    "## 2.2 평균 제곱 오차\n",
    "- 가장 많이 쓰이는 손실 함수는 평균 제곱 오차(MSE) 이다.\n",
    "![test](./img/MSE.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0] # 2번째 index로 예측\n",
    "t = [0,0,1,0,0,0,0,0,0,0] # 2 번째 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y,t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(np.array(y),np.array(t)) # MSE = 0.0975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.1,0.05,0.0,0.0,0.05,0.1,0.6,0.1,0.0,0.0] # 6번째 index로 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6975"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(np.array(y),np.array(t)) # MSE = 0.6975"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 즉, 평균 제곱 오차를 기준으로 옳바르게 예측을 할수록 MSE값이 작아짐을 알 수 있다.\n",
    "---------\n",
    "\n",
    "## 2.3 교차 엔트로피 오차\n",
    "- 또 다른 손실 함수로서 교차 엔트로피 오차도 자주 이용한다.\n",
    "![test](./img/교차엔트로피오차.png)\n",
    "- log는 밑이 e인 자연로그이다. y(k)는 신경말의 출력, t(k)는 정답레이블이다.\n",
    "- 실질적으로 정답일 때의 추정 t(k)가 1일때만 자연로그의 계산식이 된다. \n",
    "> - 즉, 교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7 # log0이 되는 것을 방지\n",
    "    return -np.sum(t*np.log(y+delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0] # 2번째 index로 예측\n",
    "t = [0,0,1,0,0,0,0,0,0,0] # 2번째 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(np.array(y),np.array(t)) # -log0.6 = 0.5108.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.1,0.05,0.0,0.0,0.05,0.1,0.6,0.1,0.0,0.0] # 6번째 index로 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.11809565095832"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(np.array(y),np.array(t)) # -log0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 평균 제곱의 오차의 판단과 일치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 미니배치 학습\n",
    "- 훈련 데이터에 대한 손실 함수의 값을 구하고, 그 값을 최대한 줄여주는 매개변수를 찾아낸다.\n",
    "> - 모든 훈련 데이터를 대상으로 손실 함수 값을 구해야 한다.\n",
    "> - 즉, 100개의 데이터가 있으면 100개의 손실 함수 값들의 합을 지표로 삼는다.\n",
    "\n",
    "![test](./img/엔트로피.png)\n",
    "\n",
    "- t(nk)는 n번째 데이터의 k번째 값을 의미\n",
    "- 마지막에 N으로 나누어 정규화를 한다. 즉, 평균 손실 함수를 구한다.\n",
    "> - 이렇게 평균을 구해 사용하면 훈련 데이터의 갯수와 상관없이 통일된 지표를 얻을 수 있다.\n",
    "- 데이터가 수백만 수천만으로 넘어가게 되면 데이터 하나하나의 손실함수를 구하는데는 당연히 시간이 오래걸릴 수 밖에 없다. 이런 경우 데이터 일부를 추려 전체의 __근사치__로 이용하는데 이렇게 훈련 데이터로부터 일부만 골라 학습을 하는 것을 미니배치라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,t_train), (X_test,t_test) = load_mnist(normalize=True,one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = X_train.shape[0] # shape 형상의 첫번째 인덱스 즉, 60,000\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size,batch_size) \n",
    "# 0 ~ train_size 사이의 데이터를 batch_size의 수 만큼 뽑는다.\n",
    "X_batch = X_train[batch_mask] # 해당 index의 행을 추출\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 (배치용) 교차엔트로피 오차 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    if y.dim == 1: # y가 1차원이라면,\n",
    "        t = t.reshape(1,t.size) \n",
    "        # 데이터 하나당 교차 엔트로피 오차를 구하는 경우 데이터의 형상을 바꿔준다. \n",
    "        y = y.reshape(1,y.size) \n",
    "    batch_size = y.shape[0] # 훈련데이터의 행 데이터의 수를 batch_size로 지정\n",
    "    return -np.sum(t*np.log(y+1e-7)) / batchsize # 엔트로피 오차의 평균을 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error_no_ont_hot(y,t):\n",
    "    if y.dim == 1:\n",
    "        t = t.reshape(1,t.size)\n",
    "        y = y.reshape(1,y_size)\n",
    "        \n",
    "    batch_size = y.shape[0] \n",
    "    return -np.sum(np.log(y[np.arange(batch_size),t]+1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- np.log(y[np.arange(batch_size),t])\n",
    "> - np.arange 0 ~ batch_size - 1 까지의 배열 생성\n",
    "> - t에는 실제 레이블이 저장되어있으므로 예를들어 [2,7,0,9,4]와 같이 저장\n",
    "> - 즉, y[0,2],y[1,7],y[2,0],.., 같이 저장\n",
    "- 이 구현의 핵심은 정답에 해당하는 신경망의 출력만으로 교차 엔트로피 오차를 계산한다는 점이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 손실함수를 사용하는 이유\n",
    "- 신경망 학습에서는 최적의 매개변수(가중치와 편향)을 탐색할 때 손실 함수의 값을 가능한 한 작게 하는 매개변수를 찾는다. \n",
    "- 이때, 매개변수의 미분을 계산하고, 그 미분 값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복한다.\n",
    "> - 즉, 가중치 매개변수의 값을 아주 조금 변화시켰을 때, 손실 함수가 어떻게 변하는지를 주목한다.\n",
    "> - 만약 미분 값이 음수이면 양의 방향으로 변화시켜 손실 함수의 값을 줄이고 미분 값이 양수이면 음의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있다.\n",
    "> - 따라서 위와 같은 방법으로 매개변수를 조정하다보면 미분값이 0에 매우 가까워지면서 어느쪽으로 움직여도 손실 함수의 값이 달라지지 않으면서 갱신은 거기서 멈추게 된다.\n",
    "- 따라서 계단함수를 활성화 함수로 사용하지 않는 이유도 대부분에서 미분을 했을때 0 이므로 손실 함수를 지표로 삼는 게 아무 의미가 없어진다.\n",
    "- 계단 함수는 한순간만 변화를 일으키지만, 시그모이드 함수이 미분(접선)은 연속적으로 변하고 곡선의 기울기도 연속적으로 변한다. 즉, 시그모이드 함수의 미분은 어느 장소라도 0이 되지 않는다.\n",
    "\n",
    "## 3.1 수치미분\n",
    "- 경사법에서는 기울기 (경사) 값을 기준으로 나아갈 방향을 정한다. \n",
    "- 미분은 '특정 순간'의 변화량을 뜻한다. 따라서 한순간의 변화량을 얻기 위한 목적이다.\n",
    "![test](./img/미분.png)\n",
    "- 결국 x의 __작은변화__가 함수 f(X)를 얼마나 변화시키느냐를 의미한다.\n",
    "> - 즉, 시간을 뜻하는 h를 한없이 0에 가깝게 한다는 의미를 limit h가 0으로 간다는 것을 나타낸다.\n",
    "\n",
    "![test](./img/차분2.png)\n",
    "- (x+h)와 x 사이의 함수 f의 차분을 계산하고 있지만, 애당초 이 계산에는 오차가 있다는 사실을 주의해야한다.\n",
    "- '진정한 미분'은 x위치의 함수의 기울기(접선)에 해당하지만, (x+h)와 x 사이의 기울기에 해당 하는 근사로 구한 접선과 차이가 크다. \n",
    "- 따라서 이 오차를 줄이기 위해 (x+h)와 (x-h)일 때의 함수 f의 차분을 계산하는 방법을 쓰기도 한다.\n",
    "> - 이 차분은 x를 중심으로 그 전후의 차분을 계산한다는 의미에서 중심 차분 혹은 중앙 차분이라 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f,x): # f 함수 / x 인수\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h)-f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0.0,20.0,0.1)\n",
    "y = function_1(x)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999999999990898"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1,5) # X = 5 일때 미분 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2999999999986347"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이렇게 계산한 미분 값이 x에 대한 f(x)의 변화량이다. 즉, 함수의 기울기에 해당\n",
    "------\n",
    "\n",
    "## 3.2 편미분\n",
    "- f(x0,x1) = Xo^2 + x1^2\n",
    "> - 변수가 2개라는 점을 유의\n",
    "![test](./img/편미분그래프.png)\n",
    "- 이와 같이 변수가 여럿인 함수에 대한 미분을 편미분이라 한다.\n",
    "- 편미분은 변수가 하나인 미분과 마찬가지로 특정 장소의 기울기를 구한다. 단, 여러 변수 중 목표 변수 하나에 초점을 맞추고 다른 변수는 값을 고정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f,x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성(원소는 모두 0)\n",
    "    \n",
    "    for idx in range(x.size): # x의 원소 개수만큼 반복\n",
    "        tmp_val = x[idx] # x의 각 원소를 tmp_val에 할당\n",
    "        x[idx] = tmp_val + h # 각 원소에 0.0001의 미세한 값을 더한다.\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h) # 미분을 하고 각 grad 원소에 할당\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2,np.array([3.0,4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 4.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2,np.array([0.0,2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 0.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2,np.array([3.0,0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이처럼 (X0,X1)의 각 점에서의 기울기를 계산할 수 있다.\n",
    "> - 앞의 코드에서 보면 점 (3,4)의 기울기는 (6,8)\n",
    "\n",
    "![test](./img/기울기.png)\n",
    "- 이 그림을 보면 기울기는 함수의 '가장 낮은 장소(최솟값)'를 가르킨다.\n",
    "> - 더 정확히 말하면 기울기가 가르키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향이다.\n",
    "\n",
    "## 3.3 경사법(경사하강법)\n",
    "- 기계학습 문제 대부분은 학습 단계에서 최적의 매개변수를 찾아낸다.\n",
    "> - 신경망 역시 최적의 매개변수 (가중치와 편향)을 학습 시에 찾아야 한다.\n",
    "> - 즉, 손실 함수가 최솟값이 될 때의 매개변수의 값이다.\n",
    "- 매개변수 공간이 광대하여 어디가 최솟값이 되는 곳인지를 짐작할 수 없지만 기울기를 잘 이용해 함수의 최솟값을 찾으려는 것이 경사법이다.\n",
    "> - 그러나 기울기가 가리키는 곳에 정말 함수의 최솟값이 있는지, 즉 그쪽이 정말로 나아갈 방향인지는 보장할 수 없다. 실제로 복잡한 함수에서는 기울기가 가리키는 방향에 최솟값이 없는 경우가 대부분이다.\n",
    ">> - 즉, 기울기가 0이 되는 지점이 한정된 범위에서의 최솟값일 수도 안정점일 수도 있다. \n",
    "- 기울어진 방향이 꼭 최솟값을 가리키는 것은 아니나, 그 방향으로 가야 함수의 값을 줄일 수 있다.\n",
    "- 경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한다. 그런 다음 이동한 곳에서도 마찬가지로 기울기를 구하고, 또 기울어진 방향으로 나아가는 것을 반복하는데 이렇게 함수의 값을 점차 줄이는 것이 경사법이다.\n",
    "\n",
    "![test](./img/경사하강법.jpg)\n",
    "- 여기서 r은 갱신하는 양을 나타낸다. 신경망에서는 이를 학습률(learning rate)라 한다.\n",
    "> - 한 번의 학습으로 얼마만큼 학습해야 할지, 즉 매개변수 값을 얼마나 갱신하느냐를 정하는 것이 학습률이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f,init_x,lr=0.01,step_num=100): \n",
    "    # f: 함수식 , init_x: 인수 , lr: 학습률, step_num: 반복 수\n",
    "    x = init_x # x_data\n",
    "    \n",
    "    for i in range(step_num): # 100번 반복\n",
    "        grad = numerical_gradient(f,x) # 편미분 즉, 기울기\n",
    "        x -= lr * grad # x - 학습률*해당 기울기 \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([-3.0,4.0]) # x_data\n",
    "gradient_descent(function_2,init_x=init_x,lr=0.1,step_num=100)\n",
    "# 함수식: y = x[0]^2+x[1]^2 , X0 = -3.0, X1 = 4.0 일때의 기울기 \n",
    "# 학습률: 0.1 , 반복 수 100 (기울기의 이동을 100번 반복)\n",
    "# 해당 기울기 : (0,0) 에 매우 근접\n",
    "# 실제로 위 함수식을 최소로 하는 값은 (0,0) 이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 갱신과정 \n",
    "![test](./img/갱신과정.png)\n",
    "- 학습률이 너무 크면 큰 값으로 발산해버리고 반대로 너무 작으면 거의 갱신되지 않은 채 중간에 끝나버린다.\n",
    "- 학습률 같은 매개변수를 하이퍼파라미터라고 하는데 신경망의 매개변수는 알고리즘에 의해서 __자동__으로 갱신되는 반면 하이퍼파라미터는 __사람이 직접 설정__해야하는 매개변수이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 신경망에서의 기울기\n",
    "- 신경망학습에서도 기울기를 구해야 하는데 여기서 말하는 기울기는 가중치 매개변수에 대한 손실 함수의 기울기이다.\n",
    "- 가중치의 각 원소를 편미분하여 각 원소들이 조금 변경했을 때 손실 함수가 얼마나 변화하느냐를 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c) # 분자 분모 양쪽에 입력값의 최대 값을 빼준다. 결과는 같다.\n",
    "    sum_exp_a = np.sum(exp_a) # 오버플로 대책\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7 # log0이 되는 것을 방지\n",
    "    return -np.sum(t*np.log(y+delta))\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) \n",
    "        # 2X3의 행렬을 정규분포로 초기화 즉, 가중치 값을 랜덤값으로 초기화\n",
    "    def predict(self,x):\n",
    "        return np.dot(x,self.W) # h(x) = XW (예측값을 출력)\n",
    "    def loss(self,x,t):\n",
    "        z = self.predict(x) # 인수 x_data predict (예측값 할당)\n",
    "        y = softmax(z) # 출력층 설계 (softmax)\n",
    "        loss = cross_entropy_error(y,t) # 손실함수 계산\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = simpleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.26629005 -1.15725025  0.18250316]\n",
      " [-0.97135901  0.93764986  1.52842581]]\n"
     ]
    }
   ],
   "source": [
    "print(net.W) # 랜덤한 가중치 값들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.71444908  0.14953472  1.48508513]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6,0.9])\n",
    "p = net.predict(x)\n",
    "print(p) # 예측 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p) # 최댓값 인덱스 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([0,0,1]) # 정답 레이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3176300942085533"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.loss(x,t) # 엔트로피 오차가 작은 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(W): # 인수 W\n",
    "    return net.loss(x,t) # 0.3176\n",
    "# f = lambda w: net.loss(x,t) 로 써도 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 여기서 정의한 f(W) 함수의 인수 W는 더미(dummy)로 만든 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04841284  0.11486401 -0.16327685]\n",
      " [ 0.07261926  0.17229601 -0.24491527]]\n"
     ]
    }
   ],
   "source": [
    "dW = numerical_gradient(f,net.W) # loss 값이 0.3176 일때의 가중치 W (2x3)을 편미분\n",
    "print(dW) # 편미분된 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- W11의 값은 대략 0.05인데 이 말은 즉, W11을 h만큼 늘리면 손실함수의 값은 0.05h만큼 증가한다는 의미이다.\n",
    "- 그래서 손실 함수를 줄인다는 관점에서는 w11은 음의방향으로 w13은 양의 방향으로 갱신해야 함을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 학습 알고리즘 구현\n",
    "- 전제\n",
    "> - 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 __학습__이라 한다.\n",
    "- 1. 미니배치\n",
    "> - 훈련 데이터 중 일부를 가져오고 선별된 미니배치의 손실 함수 값을 줄이는 것이 목표이다.\n",
    "- 2. 기울기 산출\n",
    "> - 손실 함수를 줄이기 위해 각가중치 매개변수의 기울기를 구한다. \n",
    "- 3. 매개변수 갱신\n",
    "> - 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n",
    "- 4. 반복\n",
    "> - 1 ~ 3단계를 반복한다.\n",
    "\n",
    "__데이터를 미니배치로 무작위로 선정하기 때문에 확률적 경사 하강법(SGD)라 부른다.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 2층 신경망 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x)) \n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c) # 분자 분모 양쪽에 입력값의 최대 값을 빼준다. 결과는 같다.\n",
    "    sum_exp_a = np.sum(exp_a) # 오버플로 대책\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7 # log0이 되는 것을 방지\n",
    "    return -np.sum(t*np.log(y+delta))\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self,input_size,hidden_size,output_size,weight_init_std=0.01):\n",
    "        # __init__ 클래스를 초기화 (초기화 메서드는 클래스를 생성할 때 불리는 메서드이다.)\n",
    "        # input_size: 입력층 노드 설정\n",
    "        # hidden_size: 은닉층 노드 설정\n",
    "        # output_size: 출력층 노드 설정\n",
    "        # weight_init_std: 학습률\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size,hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size) # 편향은 0으로 초기화\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size,output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        # params: 딕셔너리 변수\n",
    "        # 신경망의 매개변수(W,b)를 보관\n",
    "        # 매개변수를 랜덤값으로 초기화 하고 학습률을 곱한다.\n",
    "        # np.zeros(hidden_size) 은닉층 노드의 갯수만큼 1차원 원소 생성\n",
    "        \n",
    "    def predict(self,x):\n",
    "        W1,W2 = self.params['W1'], self.params['W2']\n",
    "        b1,b2 = self.params['b1'], self.params['b2']\n",
    "        # 딕셔너리 변수에 보관된 매개변수들을 각 변수에 할당\n",
    "        \n",
    "        a1 = np.dot(x,W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1,W2) + b2\n",
    "        y = softmax(a2)\n",
    "        # 층에서 층으로 신호 전달 (즉, 연산)\n",
    "        return y\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        y = self.predict(x) # 예측값\n",
    "        \n",
    "        return cross_entropy_error(y,t) # 엔트로피 오차 계산 즉, loss값 return\n",
    "    \n",
    "    def accuracy(self,x,t):\n",
    "        y = self.predict(x) # 예측값\n",
    "        y = np.argmax(y,axis=1)\n",
    "        t = np.argmax(t,axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0]) # 정확도 계산\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self,x,t):\n",
    "        loss_W  = lambda W: self.loss(x,t) # loss 값 할당\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W,self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W,self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W,self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W,self.params['b2'])\n",
    "        # 각 매개변수의 편미분(기울기)을 구해서 grads변수에 보관\n",
    "        \n",
    "        return grads #return\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100,output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TwoLayerNet 클래스는 딕셔너리인 params와 grads를 인스턴스 변수로 갖는다.\n",
    "- params 변수에는 이 신경망에 필요한 매개변수가 모두 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100,784) # x_data 100,784 즉, 100개의 데이터 784개의 특성을 지님\n",
    "y = net.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00100185, 0.00102833, 0.00105191, 0.00099501, 0.00108478,\n",
       "        0.00098234, 0.001053  , 0.00096194, 0.00094275, 0.00089434],\n",
       "       [0.00100588, 0.00102729, 0.00105604, 0.00099692, 0.00109099,\n",
       "        0.00098168, 0.00104923, 0.00096294, 0.00094166, 0.00089309],\n",
       "       [0.00100606, 0.00102912, 0.00105743, 0.00099613, 0.00108851,\n",
       "        0.00098444, 0.00105285, 0.00095963, 0.00094134, 0.0008941 ],\n",
       "       [0.00100417, 0.00103073, 0.00105369, 0.0009948 , 0.00108816,\n",
       "        0.00098111, 0.00104874, 0.0009631 , 0.00094131, 0.00089417],\n",
       "       [0.00100409, 0.00103051, 0.00105515, 0.0009958 , 0.0010888 ,\n",
       "        0.00098411, 0.00104951, 0.00095916, 0.00093852, 0.00089496],\n",
       "       [0.00100499, 0.00102994, 0.00105107, 0.00100009, 0.00109062,\n",
       "        0.00098233, 0.00105173, 0.00096589, 0.00094093, 0.00088936],\n",
       "       [0.00100296, 0.00102716, 0.00105476, 0.00099905, 0.00108979,\n",
       "        0.00098374, 0.00105227, 0.00096216, 0.00094032, 0.00089118],\n",
       "       [0.00100621, 0.00102922, 0.00105309, 0.00099681, 0.00108945,\n",
       "        0.00097943, 0.00104583, 0.00096244, 0.00094107, 0.00089478],\n",
       "       [0.00100324, 0.00102965, 0.00105564, 0.00099519, 0.00108346,\n",
       "        0.00098239, 0.00104955, 0.00096435, 0.00094108, 0.00089165],\n",
       "       [0.00100592, 0.00102858, 0.0010529 , 0.00099228, 0.00108926,\n",
       "        0.00098646, 0.00105228, 0.00096234, 0.00094073, 0.00089251],\n",
       "       [0.00100874, 0.00102991, 0.0010563 , 0.00099694, 0.0010927 ,\n",
       "        0.00098324, 0.00104827, 0.00096355, 0.00093948, 0.00089219],\n",
       "       [0.00100803, 0.00102954, 0.00105404, 0.00099237, 0.00108763,\n",
       "        0.00098265, 0.001048  , 0.00096381, 0.00094121, 0.00089313],\n",
       "       [0.00100642, 0.00103078, 0.00105215, 0.00099704, 0.00108777,\n",
       "        0.00098732, 0.00105086, 0.00095748, 0.00094173, 0.00089602],\n",
       "       [0.00100169, 0.00102699, 0.00105339, 0.00099666, 0.00108544,\n",
       "        0.00098505, 0.00105265, 0.00096216, 0.00094255, 0.00089223],\n",
       "       [0.00100382, 0.00102844, 0.00105467, 0.00099951, 0.00108812,\n",
       "        0.00098417, 0.00105098, 0.00095843, 0.00093541, 0.00089393],\n",
       "       [0.00100592, 0.00102715, 0.00105102, 0.00099282, 0.00108469,\n",
       "        0.0009824 , 0.00105292, 0.0009631 , 0.0009417 , 0.00088845],\n",
       "       [0.00100473, 0.00102928, 0.00105472, 0.00099347, 0.0010877 ,\n",
       "        0.00098207, 0.00104982, 0.00096583, 0.00093893, 0.00089397],\n",
       "       [0.00100577, 0.00102911, 0.00105096, 0.00099399, 0.00108787,\n",
       "        0.0009821 , 0.00105176, 0.0009627 , 0.00094339, 0.00089159],\n",
       "       [0.00100623, 0.0010256 , 0.00105383, 0.00099334, 0.00109037,\n",
       "        0.00098025, 0.00105044, 0.00096252, 0.00093811, 0.00089197],\n",
       "       [0.00100131, 0.00103009, 0.00104995, 0.00099566, 0.0010896 ,\n",
       "        0.00098507, 0.00105006, 0.00096077, 0.00094147, 0.00089082],\n",
       "       [0.00100417, 0.00102636, 0.00105611, 0.00099415, 0.00109015,\n",
       "        0.00098432, 0.00104774, 0.0009635 , 0.00093922, 0.00089379],\n",
       "       [0.00100497, 0.00102959, 0.00105275, 0.00099345, 0.00108746,\n",
       "        0.00098139, 0.00105   , 0.00096499, 0.00093961, 0.00089307],\n",
       "       [0.00100396, 0.00102722, 0.00105048, 0.00099481, 0.00108954,\n",
       "        0.00098148, 0.00104784, 0.00096428, 0.0009441 , 0.00089051],\n",
       "       [0.00100271, 0.00102984, 0.00105265, 0.00099016, 0.00108343,\n",
       "        0.00098237, 0.001049  , 0.00096932, 0.00094198, 0.00089151],\n",
       "       [0.00100267, 0.00103133, 0.00105146, 0.00099079, 0.00108987,\n",
       "        0.0009834 , 0.00105178, 0.000965  , 0.00094598, 0.00088718],\n",
       "       [0.00100403, 0.00103125, 0.00105263, 0.00100018, 0.00109152,\n",
       "        0.0009811 , 0.00105306, 0.00096684, 0.00094262, 0.00089261],\n",
       "       [0.00100405, 0.00102668, 0.00105166, 0.00099882, 0.00108937,\n",
       "        0.00097987, 0.00104957, 0.00096387, 0.00093996, 0.0008914 ],\n",
       "       [0.00100752, 0.00102816, 0.0010519 , 0.00100089, 0.00108934,\n",
       "        0.00098163, 0.00105132, 0.00096071, 0.0009413 , 0.00089082],\n",
       "       [0.00100346, 0.00103204, 0.00105033, 0.00099257, 0.00108725,\n",
       "        0.00098008, 0.00105053, 0.00096521, 0.00094215, 0.0008909 ],\n",
       "       [0.00100581, 0.00103185, 0.00104883, 0.00099732, 0.00109101,\n",
       "        0.00097782, 0.00105101, 0.00096646, 0.00094033, 0.00088973],\n",
       "       [0.00100202, 0.00102837, 0.00105166, 0.00099794, 0.00108867,\n",
       "        0.00098378, 0.00105422, 0.00096148, 0.00094489, 0.00088922],\n",
       "       [0.00100405, 0.00102869, 0.00105272, 0.00099488, 0.00108807,\n",
       "        0.00098423, 0.00105088, 0.00096244, 0.00094043, 0.00089073],\n",
       "       [0.00100697, 0.00103074, 0.00105739, 0.00099654, 0.00108918,\n",
       "        0.0009856 , 0.00105257, 0.0009668 , 0.00094214, 0.00089351],\n",
       "       [0.00100447, 0.00103009, 0.00104961, 0.0009971 , 0.00108819,\n",
       "        0.00097977, 0.00105049, 0.00096432, 0.00094158, 0.00089171],\n",
       "       [0.00100301, 0.00103311, 0.00104903, 0.00099629, 0.00108866,\n",
       "        0.00097928, 0.00105004, 0.00096368, 0.00094123, 0.00089245],\n",
       "       [0.00100556, 0.00103119, 0.00105681, 0.00099618, 0.00109108,\n",
       "        0.00098249, 0.00105119, 0.00096166, 0.00093861, 0.00089294],\n",
       "       [0.00100512, 0.00102584, 0.00105524, 0.00099273, 0.00108824,\n",
       "        0.00098648, 0.00104703, 0.00096186, 0.00094109, 0.00089194],\n",
       "       [0.00100847, 0.00103215, 0.00105244, 0.00099518, 0.00109038,\n",
       "        0.00098256, 0.00104714, 0.0009636 , 0.00093643, 0.00089368],\n",
       "       [0.00100676, 0.00102735, 0.00105775, 0.00099454, 0.00108572,\n",
       "        0.00098303, 0.00104811, 0.00095812, 0.00094146, 0.0008928 ],\n",
       "       [0.00100501, 0.00102889, 0.00105398, 0.00099349, 0.00108752,\n",
       "        0.00098523, 0.00104952, 0.00096074, 0.00094189, 0.0008929 ],\n",
       "       [0.00100368, 0.00102979, 0.00105303, 0.00099724, 0.00108616,\n",
       "        0.00098094, 0.0010523 , 0.00096266, 0.00094039, 0.00089373],\n",
       "       [0.00100293, 0.00102413, 0.00105421, 0.00099492, 0.00108955,\n",
       "        0.00097978, 0.00105019, 0.00096093, 0.00094095, 0.00089259],\n",
       "       [0.00100563, 0.00102874, 0.00105066, 0.0009995 , 0.00108868,\n",
       "        0.00098145, 0.00105247, 0.00096182, 0.00094027, 0.00089214],\n",
       "       [0.00100459, 0.00102926, 0.00105422, 0.00099625, 0.00108955,\n",
       "        0.00098379, 0.00105368, 0.00096381, 0.00093793, 0.00089238],\n",
       "       [0.00100776, 0.00102921, 0.00105362, 0.00099379, 0.00108578,\n",
       "        0.00098425, 0.00105029, 0.00096408, 0.00094412, 0.00089418],\n",
       "       [0.00100787, 0.00102833, 0.00105553, 0.00099615, 0.00109116,\n",
       "        0.0009828 , 0.0010484 , 0.00096065, 0.00094166, 0.00088994],\n",
       "       [0.00100456, 0.0010275 , 0.00105641, 0.00099664, 0.00109114,\n",
       "        0.00098212, 0.00104985, 0.00096359, 0.00093881, 0.00088928],\n",
       "       [0.0010048 , 0.00102982, 0.00105476, 0.00099368, 0.00108676,\n",
       "        0.00097796, 0.00105034, 0.00096083, 0.00094264, 0.00089196],\n",
       "       [0.00100422, 0.00102382, 0.00105589, 0.00099135, 0.00108752,\n",
       "        0.00098297, 0.00105006, 0.00096434, 0.00094225, 0.00089318],\n",
       "       [0.00100485, 0.00102725, 0.00105133, 0.0009975 , 0.00108394,\n",
       "        0.00098566, 0.00105399, 0.00096087, 0.00094355, 0.0008911 ],\n",
       "       [0.00100501, 0.00102676, 0.0010507 , 0.00099396, 0.00109037,\n",
       "        0.00098166, 0.00104742, 0.00096597, 0.00094177, 0.00089342],\n",
       "       [0.00100824, 0.00103327, 0.00105362, 0.00099886, 0.00108725,\n",
       "        0.00098533, 0.00105248, 0.00096503, 0.00094239, 0.0008875 ],\n",
       "       [0.0010031 , 0.00102634, 0.00105452, 0.00099358, 0.00108891,\n",
       "        0.00098035, 0.00104945, 0.00096427, 0.00093921, 0.00089616],\n",
       "       [0.00100184, 0.00102945, 0.00105765, 0.00099472, 0.00108871,\n",
       "        0.00098433, 0.00104939, 0.00096207, 0.00093977, 0.00089661],\n",
       "       [0.00100342, 0.00102976, 0.00105107, 0.00099511, 0.00108935,\n",
       "        0.00098416, 0.0010537 , 0.00096165, 0.00093721, 0.00088893],\n",
       "       [0.00100251, 0.0010285 , 0.00105064, 0.00099342, 0.0010916 ,\n",
       "        0.00098138, 0.0010482 , 0.00096394, 0.00094062, 0.00089135],\n",
       "       [0.0010027 , 0.00102875, 0.00105153, 0.00099594, 0.0010865 ,\n",
       "        0.0009844 , 0.00105089, 0.00096289, 0.0009411 , 0.00089107],\n",
       "       [0.00100382, 0.00102727, 0.00105501, 0.00099686, 0.00108688,\n",
       "        0.00098089, 0.00104897, 0.00096259, 0.00094187, 0.00089458],\n",
       "       [0.00100382, 0.00102828, 0.00104969, 0.00099507, 0.00108335,\n",
       "        0.00098219, 0.00105318, 0.00096223, 0.0009452 , 0.00089182],\n",
       "       [0.00100546, 0.00102974, 0.0010509 , 0.00099295, 0.00108606,\n",
       "        0.00098142, 0.00105287, 0.00096377, 0.00094198, 0.00089037],\n",
       "       [0.00100334, 0.00103344, 0.00105108, 0.00099628, 0.00108737,\n",
       "        0.0009805 , 0.00105303, 0.00096011, 0.00094152, 0.00088771],\n",
       "       [0.00101138, 0.00103343, 0.0010495 , 0.00099459, 0.00109108,\n",
       "        0.00098618, 0.0010481 , 0.00095863, 0.00093765, 0.00089289],\n",
       "       [0.00100426, 0.00102813, 0.00105273, 0.00099627, 0.00108832,\n",
       "        0.00098168, 0.00105144, 0.00096243, 0.00094406, 0.0008922 ],\n",
       "       [0.00100468, 0.00102979, 0.00105693, 0.00099803, 0.0010913 ,\n",
       "        0.00098119, 0.00105445, 0.00096297, 0.00094551, 0.0008907 ],\n",
       "       [0.00100587, 0.00103035, 0.00105433, 0.00100083, 0.00108889,\n",
       "        0.00098025, 0.00105405, 0.00096147, 0.00094235, 0.00088906],\n",
       "       [0.00100886, 0.00102571, 0.00105682, 0.00099689, 0.00108557,\n",
       "        0.00097869, 0.00104836, 0.00096129, 0.00094047, 0.00089426],\n",
       "       [0.00100301, 0.00102704, 0.0010552 , 0.00099696, 0.0010861 ,\n",
       "        0.00098453, 0.0010486 , 0.00095844, 0.00093987, 0.00089275],\n",
       "       [0.00100764, 0.00102633, 0.00105308, 0.00099548, 0.00108868,\n",
       "        0.00098115, 0.00104422, 0.00096281, 0.00094295, 0.00089251],\n",
       "       [0.00100375, 0.00102966, 0.00105504, 0.00099998, 0.00108685,\n",
       "        0.00098195, 0.0010532 , 0.00096556, 0.00094376, 0.0008895 ],\n",
       "       [0.00100199, 0.00102753, 0.00105275, 0.00099485, 0.00108728,\n",
       "        0.0009797 , 0.00104826, 0.0009612 , 0.00093939, 0.00089197],\n",
       "       [0.00100367, 0.00102841, 0.00105039, 0.00099664, 0.00109065,\n",
       "        0.00098105, 0.00105418, 0.00095925, 0.00093957, 0.00089461],\n",
       "       [0.00100313, 0.00103389, 0.00105018, 0.00099269, 0.00109141,\n",
       "        0.00098217, 0.00105066, 0.00096365, 0.00094208, 0.00089066],\n",
       "       [0.00100188, 0.00102828, 0.0010537 , 0.00099654, 0.00108574,\n",
       "        0.00098231, 0.00105023, 0.00096335, 0.0009425 , 0.00089067],\n",
       "       [0.00100445, 0.00102748, 0.0010499 , 0.00099605, 0.00109417,\n",
       "        0.00098105, 0.00105056, 0.00096076, 0.00094198, 0.00089298],\n",
       "       [0.00100304, 0.00102889, 0.00105394, 0.00099559, 0.00108952,\n",
       "        0.00098714, 0.00105438, 0.00096477, 0.00094427, 0.00089091],\n",
       "       [0.00100661, 0.00102934, 0.00104814, 0.00099888, 0.00109098,\n",
       "        0.00097976, 0.00105385, 0.00096257, 0.00094186, 0.00089327],\n",
       "       [0.00100307, 0.00102532, 0.00105262, 0.00099103, 0.00108525,\n",
       "        0.00098483, 0.00104963, 0.00096361, 0.00093918, 0.00089444],\n",
       "       [0.00100331, 0.00102794, 0.0010527 , 0.00099539, 0.00108938,\n",
       "        0.00098246, 0.00105042, 0.00096054, 0.00094322, 0.00089208],\n",
       "       [0.00100469, 0.00102763, 0.00105617, 0.00099639, 0.00108757,\n",
       "        0.00098551, 0.00105325, 0.0009661 , 0.00094202, 0.00088995],\n",
       "       [0.0010073 , 0.00102445, 0.00105175, 0.00099691, 0.00108939,\n",
       "        0.00098248, 0.00105261, 0.0009597 , 0.00093942, 0.00089271],\n",
       "       [0.00100571, 0.00103143, 0.00105248, 0.00099253, 0.00108665,\n",
       "        0.00098254, 0.00104964, 0.0009625 , 0.00094255, 0.00088938],\n",
       "       [0.00100384, 0.00103148, 0.00105623, 0.00099646, 0.00108546,\n",
       "        0.00098281, 0.00105294, 0.00096378, 0.00094109, 0.00089254],\n",
       "       [0.00100545, 0.00102998, 0.0010529 , 0.00099353, 0.0010881 ,\n",
       "        0.00098191, 0.00105517, 0.00096671, 0.00093932, 0.00089222],\n",
       "       [0.00100103, 0.00102878, 0.0010549 , 0.00099497, 0.00108842,\n",
       "        0.00098096, 0.00105001, 0.00096259, 0.00093901, 0.00089534],\n",
       "       [0.00100657, 0.00103061, 0.00105144, 0.00099701, 0.00108591,\n",
       "        0.00097865, 0.00104799, 0.00096222, 0.0009379 , 0.00089234],\n",
       "       [0.00100219, 0.00103179, 0.00105012, 0.00099615, 0.00109669,\n",
       "        0.00098051, 0.00104975, 0.00096549, 0.00094223, 0.0008927 ],\n",
       "       [0.00100492, 0.00103214, 0.00105018, 0.00099682, 0.00109017,\n",
       "        0.00097856, 0.00104755, 0.00096441, 0.00093977, 0.00089184],\n",
       "       [0.00100272, 0.00102731, 0.00105077, 0.00099836, 0.00108771,\n",
       "        0.0009857 , 0.00105019, 0.00096295, 0.0009409 , 0.00088708],\n",
       "       [0.00100223, 0.00102914, 0.00105269, 0.00099475, 0.00109082,\n",
       "        0.000985  , 0.00105289, 0.00096166, 0.00094378, 0.00088818],\n",
       "       [0.00100888, 0.00102777, 0.00105556, 0.00099468, 0.00109238,\n",
       "        0.00097832, 0.00104844, 0.00096323, 0.00093897, 0.0008898 ],\n",
       "       [0.00100377, 0.00103172, 0.0010472 , 0.00099673, 0.00108685,\n",
       "        0.00098164, 0.00105223, 0.00096145, 0.00094485, 0.00088796],\n",
       "       [0.00100561, 0.00103286, 0.00104925, 0.00099816, 0.0010903 ,\n",
       "        0.00098337, 0.001053  , 0.00096277, 0.00094112, 0.00089178],\n",
       "       [0.0010083 , 0.00103189, 0.00105569, 0.00099701, 0.00108943,\n",
       "        0.00097902, 0.00104933, 0.00096522, 0.00093987, 0.00089326],\n",
       "       [0.00100552, 0.00102874, 0.00105349, 0.00099453, 0.00109074,\n",
       "        0.00098429, 0.00105043, 0.00096306, 0.0009385 , 0.00089144],\n",
       "       [0.00100724, 0.00102926, 0.00105407, 0.00099155, 0.00109104,\n",
       "        0.00098116, 0.00105019, 0.0009646 , 0.00094007, 0.00089302],\n",
       "       [0.00100371, 0.0010293 , 0.00105053, 0.00099308, 0.00109224,\n",
       "        0.00098182, 0.00105053, 0.00096352, 0.00093975, 0.00089126],\n",
       "       [0.0010047 , 0.00102833, 0.00104518, 0.00099502, 0.00109211,\n",
       "        0.00098323, 0.0010506 , 0.00096022, 0.00094252, 0.00088881],\n",
       "       [0.00100244, 0.00103023, 0.00105273, 0.00099523, 0.00108818,\n",
       "        0.00098193, 0.00105069, 0.00096216, 0.00094106, 0.00089526],\n",
       "       [0.00100345, 0.00102755, 0.001054  , 0.00099706, 0.00108806,\n",
       "        0.000983  , 0.00105376, 0.00096015, 0.00094411, 0.00088992],\n",
       "       [0.00100649, 0.0010284 , 0.00105237, 0.00099579, 0.00109389,\n",
       "        0.00098314, 0.00104992, 0.00096432, 0.00093876, 0.00089031]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100,784)\n",
    "t = np.random.rand(100,10)\n",
    "\n",
    "grads = net.numerical_gradient(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print(grads['W1'].shape)\n",
    "print(grads['b1'].shape)\n",
    "print(grads['W2'].shape)\n",
    "print(grads['b2'].shape) # 각 매개변수의 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 미니배치 학습 구현하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train,y_train), (x_test,y_test) = load_mnist(normalize=True,one_hot_label=True)\n",
    "\n",
    "train_loss_list = [] # 손실 함수 값을 담을 list\n",
    "\n",
    "# 하이퍼 파라미터\n",
    "iters_num = 10000 # 반복 횟수\n",
    "train_size = x_train.shape[0] # train_size\n",
    "batch_size = 100 # 미니배치 크기\n",
    "learning_rate = 0.1 # learning rate\n",
    "\n",
    "network = TwoLayerNet(input_size=784,hidden_size=50,output_size=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iters_num): # 10,000번 반복\n",
    "    batch_mask = np.random.choice(train_size,batch_size) \n",
    "    # batch_size만큼 훈련 셋에서 인덱스 추출\n",
    "    x_batch = x_train[batch_mask] # 인덱스 번호에 있는 데이터 할당\n",
    "    t_batch = t_train[batch_mask] \n",
    "    \n",
    "    grad = network.numerical_gradient(x_batch,t_batch) # 기울기 계산\n",
    "    \n",
    "    for key in ('W1','b1','W2','b2'): # 매개변수 갱신\n",
    "        network.params[key] -= learning_rate * grad[key] # x - 기울기*학습률\n",
    "        \n",
    "    loss = network.loss(x_batch,t_batch) # loss 값 계산\n",
    "    train_loss_list.append(loss) # loss 값 리스트에 추가\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 매번 60,000개의 데이터 중 임의로 100개의 데이터를 추려내고 확률적 경사하강법을 진행하여 매개변수를 갱신\n",
    "- 학습 횟구가 늘어가면서 손실 함수의 값이 줄어든다.\n",
    "> - 이는 학습이 잘 되고 있다는 뜻으로, 신경망의 가중치 매개변수가 서서히 데이터에 적응하고 있음을 의미한다.\n",
    "\n",
    "## 시험 데이터로 평가\n",
    "- 범용 능력을 평가하기 위해 훈련데이터에 포함되지 않은 데이터를 사용해 평가한다.\n",
    "- 이를 위해 학습도중 정기적으로 훈련 데이터와 시험 데이터를 대상으로 정확도를 기록한다.\n",
    "> - 여기에서는 1에폭별로 훈련 데이터와 시험 데이터에 대한 정확도를 기록\n",
    ">> - 1에폭은 학습에서 훈련 데이터를 모두 소진했을 때의 횟수에 해당한다.\n",
    ">> - 예를들어 10,000개의 데이터를 100개씩 미니배치 할 경우 SGD하강법을 100회 반복하면 모든 훈련 데이터를 소진한게 되는데 이 경우 100회가 1에폭이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train,y_train), (x_test,y_test) = load_mnist(normalize=True,one_hot_label=True)\n",
    "\n",
    "train_loss_list = [] # 손실 함수 값을 담을 list\n",
    "train_acc_list = [] # train 정확도 값을 담을 list\n",
    "test_acc_list = [] # test 정확도 값을 담을 list\n",
    "\n",
    "iter_per_epoch = max(train_size/batch_size,1) \n",
    "# 1에폭 당 반복 수 : 600\n",
    "\n",
    "# 하이퍼 파라미터\n",
    "iters_num = 10000 # 반복 횟수\n",
    "train_size = x_train.shape[0] # train_size\n",
    "batch_size = 100 # 미니배치 크기\n",
    "learning_rate = 0.1 # learning rate\n",
    "\n",
    "network = TwoLayerNet(input_size=784,hidden_size=50,output_size=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc =0.10441666666666667,0.1028\n"
     ]
    }
   ],
   "source": [
    "for i in range(iters_num): # 10,000번 반복\n",
    "    batch_mask = np.random.choice(train_size,batch_size) \n",
    "    # batch_size만큼 훈련 셋에서 인덱스 추출\n",
    "    x_batch = x_train[batch_mask] # 인덱스 번호에 있는 데이터 할당\n",
    "    t_batch = t_train[batch_mask] \n",
    "    \n",
    "    grad = network.numerical_gradient(x_batch,t_batch) # 기울기 계산\n",
    "    \n",
    "    for key in ('W1','b1','W2','b2'): # 매개변수 갱신\n",
    "        network.params[key] -= learning_rate * grad[key] # x - 기울기*학습률\n",
    "        \n",
    "    loss = network.loss(x_batch,t_batch) # loss 값 계산\n",
    "    train_loss_list.append(loss) # loss 값 리스트에 추가\n",
    "    \n",
    "    if i % iter_per_epoch == 0: \n",
    "        # 1epoch 당 (매개변수가 1epoch 동안 조정)\n",
    "        train_acc = network.accuracy(x_train,t_train) # train 정확도\n",
    "        test_acc = network.accuracy(x_test,t_test) # test 정확도\n",
    "        train_acc_list.append(train_acc) # 리스트에 추가\n",
    "        test_acc_list.append(test_acc) # 리스트에 추가\n",
    "        print(\"train acc, test acc = \" + str(train_acc) + ',' + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1epoch 마다 모든 훈련 데이터와 시험 데이터에 대한 정확도를 계산하고, 그 결과를 기록한다.\n",
    "- 에폭이 진행될수록(학습이 진행될수록) 훈련 데이터와 시험 데이터를 사용하고 평가한 정확도가 좋아지고 있다. 또, 두 정확도에는 차이가 거의 없음을 알 수 있다. 즉, 오버피팅이 일어나지 않았다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
